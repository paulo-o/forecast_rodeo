{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get KNN neighbor predictions\n",
    "\n",
    "Generates the predictions of the most similar viable neighbors for all dates based on saved KNN similarities (generated by knn_step_1-compute_similarities.ipynb) and saves the predictions to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Package loading\n",
    "\n",
    "# Autoreload packages that are modified\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Plotting magic\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load relevant packages\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "import sys\n",
    "import subprocess\n",
    "from datetime import datetime, timedelta\n",
    "import netCDF4\n",
    "import time\n",
    "from functools import partial\n",
    "import os\n",
    "\n",
    "if os.path.basename(os.getcwd()) == \"experiments\":\n",
    "    os.chdir(os.path.join(\"..\",\"..\"))\n",
    "\n",
    "# Adds 'experiments' folder to path to load experiments_util\n",
    "sys.path.insert(0, 'src/experiments')\n",
    "# Load general utility functions\n",
    "from experiments_util import *\n",
    "# Load functionality for fitting and predicting\n",
    "from fit_and_predict import *\n",
    "# Load functionality for evaluation\n",
    "from skill import *\n",
    "# Load functionality for knn\n",
    "from knn_util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output experiment name determined by inputs above\n",
    "experiment = \"knn\"\n",
    "# the variable to be predicted\n",
    "gt_id = \"contest_precip\" # \"contest_precip\" or \"contest_tmp2m\"\n",
    "# Prediction horizon\n",
    "target_horizon = \"56w\" # \"34w\" or \"56w\"\n",
    "# The number of past days that should contribute to measure of similarity\n",
    "past_days = 60\n",
    "# Only use measurements available this many days prior to \n",
    "# official contest submission date\n",
    "days_early = 365 - (14 + get_forecast_delta(target_horizon, days_early = 0)) \n",
    "print days_early\n",
    "# Maximum number of neighbors\n",
    "max_num_nbrs = 20\n",
    "\n",
    "## Process inputs\n",
    "\n",
    "# Identify measurement variable name\n",
    "measurement_variable = get_measurement_variable(gt_id) # 'tmp2m' or 'prate'\n",
    "\n",
    "# column names for gt_col, clim_col and anom_col \n",
    "gt_col = measurement_variable\n",
    "clim_col = measurement_variable+\"_clim\"\n",
    "anom_col = get_measurement_variable(gt_id)+\"_anom\" # 'tmp2m_anom' or 'prate_anom'\n",
    "\n",
    "# nbr_start_delta = minimum number of days between start date of most recent neighbor to consider\n",
    "# (aggregation_days = 2 weeks to observe complete measurement) and start date of target period \n",
    "# (2 or 4 weeks plus days early days ahead)\n",
    "aggregation_days = 14\n",
    "nbr_start_delta = (aggregation_days + \n",
    "                   get_forecast_delta(target_horizon, days_early = days_early))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Load ground truth anomalies (using complete climatology)\n",
    "#\n",
    "anoms = get_lat_lon_date_features(anom_ids = [gt_id], first_year=get_first_year(gt_id))\n",
    "# Drop unnecessary columns\n",
    "anoms = anoms.loc[:,['lat','lon','start_date',gt_col,anom_col,clim_col]]\n",
    "# Pivot dataframe to have one row per start date\n",
    "tic(); anoms = anoms.set_index(['lat','lon','start_date']).unstack(['lat','lon']); toc()\n",
    "# Drop start dates that have no measurements (e.g., leap days, which have no climatology)\n",
    "anoms = anoms.dropna(axis='index', how='all')\n",
    "tic()\n",
    "# Determine which neighbor start_dates are viable\n",
    "viable_neighbors = anoms.index\n",
    "# Stack anoms dataframe to have lat, lon, start_date columns\n",
    "anoms = anoms.stack(['lat','lon']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read viable similarities from disk\n",
    "cache_dir = os.path.join(\"results\", experiment, \"shared\")\n",
    "viable_similarities_file = os.path.join(\n",
    "    cache_dir,'viable_similarities-{}-{}-days{}-early{}.h5'.format(gt_id,target_horizon,past_days,days_early))\n",
    "print \"Reading viable similarities from \"+viable_similarities_file; tic()\n",
    "viable_similarities = pd.read_hdf(viable_similarities_file); toc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Form and save neighbor predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Form predictions\n",
    "#\n",
    "\n",
    "# Prepare dataframes for storing predictions and similarities\n",
    "preds = pd.DataFrame(columns = ['lat','lon','start_date']+['knn'+str(i+1) for i in range(max_num_nbrs)])\n",
    "\n",
    "# Target dates are dates for which viable similarities are not all NaN\n",
    "all_target_dates = viable_similarities.loc[~viable_similarities.isnull().all(axis=1)].index\n",
    "\n",
    "# Process results from each year\n",
    "for target in all_target_dates:\n",
    "    # Find the neighbors\n",
    "    nbrs = get_target_neighbors(\n",
    "        target, target_horizon, gt_id, \n",
    "        nbr_start_delta, past_days, viable_similarities, False,\n",
    "        False)[0:max_num_nbrs]\n",
    "\n",
    "    if nbrs.size != max_num_nbrs:\n",
    "        continue\n",
    "        \n",
    "    # Get predictions of each neighbor\n",
    "    nbr_preds = anoms.loc[anoms.start_date.isin(nbrs), ['lat','lon','start_date',anom_col]].copy()\n",
    "    nbr_preds_wide = nbr_preds.pivot_table(index=['lat','lon'], columns='start_date')\n",
    "    nbr_dates = nbr_preds_wide.columns.levels[1]\n",
    "    nbr_preds_wide = pd.DataFrame(nbr_preds_wide.to_records())\n",
    "    nbr_preds_wide.columns = ['lat','lon'] + nbr_dates.tolist()\n",
    "    \n",
    "    # Reorder columns in order of most similar to least similar neighbor\n",
    "    nbr_preds_wide = nbr_preds_wide.loc[:,['lat','lon'] + nbrs.tolist()]\n",
    "    nbr_preds_wide.columns = ['lat','lon'] + ['knn'+str(i+1) for i in range(max_num_nbrs)]\n",
    "    \n",
    "    # Associate with target date\n",
    "    nbr_preds_wide['start_date'] = target\n",
    "    \n",
    "    # Store predictions\n",
    "    preds = preds.append(nbr_preds_wide)\n",
    "    \n",
    "    if target.month == 1 and target.day == 1:\n",
    "        print target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to file\n",
    "cache_dir = os.path.join('results', experiment)\n",
    "if not os.path.isdir(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "preds_file = os.path.join(\n",
    "    cache_dir,'knn-{}-{}-days{}-early{}-maxnbrs{}.h5'.format(gt_id,target_horizon,past_days,days_early,max_num_nbrs))\n",
    "print \"Saving predictions to \"+preds_file; tic()\n",
    "preds.to_hdf(preds_file, key=\"data\", mode=\"w\"); toc()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
