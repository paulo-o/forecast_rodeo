{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regression for 2011-2018 target dates\n",
    "\n",
    "Carry out regression experiment for a fixed set of predictors and all 2011-2018 target dates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoreload packages that are modified\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Plotting magic\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load relevant packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "import sys\n",
    "import subprocess\n",
    "from datetime import datetime, timedelta\n",
    "import netCDF4\n",
    "import time\n",
    "from functools import partial\n",
    "import os\n",
    "\n",
    "# Ensure that working directory is forecast_rodeo\n",
    "if os.path.basename(os.getcwd()) == \"experiments\":\n",
    "    # Navigate to forecast_rodeo\n",
    "    os.chdir(os.path.join(\"..\",\"..\"))\n",
    "if os.path.basename(os.getcwd()) != \"forecast_rodeo\":\n",
    "    raise Exception(\"You must be in the forecast_rodeo folder\")\n",
    "\n",
    "# Adds 'experiments' folder to path to load experiments_util\n",
    "sys.path.insert(0, 'src/experiments')\n",
    "# Load general utility functions\n",
    "from experiments_util import *\n",
    "# Load functionality for fitting and predicting\n",
    "from fit_and_predict import *\n",
    "# Load functionality for evaluation\n",
    "from skill import *\n",
    "# Load stepwise utility functions\n",
    "from stepwise_util import *\n",
    "\n",
    "# Experiment name\n",
    "experiment = \"regression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Choose target\n",
    "#\n",
    "gt_id = \"contest_precip\" # \"contest_precip\" or \"contest_tmp2m\"\n",
    "target_horizon = \"56w\" # \"34w\" or \"56w\"\n",
    "\n",
    "#\n",
    "# Set variables based on target choice\n",
    "#\n",
    "\n",
    "# Identify measurement variable name\n",
    "measurement_variable = get_measurement_variable(gt_id) # 'tmp2m' or 'precip'\n",
    "\n",
    "# column names for gt_col, clim_col and anom_col \n",
    "gt_col = measurement_variable\n",
    "clim_col = measurement_variable+\"_clim\"\n",
    "anom_col = get_measurement_variable(gt_id)+\"_anom\" # 'tmp2m_anom' or 'precip_anom'\n",
    "\n",
    "# anom_inv_std_col: column name of inverse standard deviation of anomalies for each start_date\n",
    "anom_inv_std_col = anom_col+\"_inv_std\"\n",
    "\n",
    "# Name of knn columns\n",
    "knn_cols = [\"knn\"+str(ii) for ii in xrange(1,21)]\n",
    "\n",
    "#\n",
    "# Create list of official contest submission dates in YYYYMMDD format\n",
    "#\n",
    "submission_dates = [datetime(y,4,18)+timedelta(14*i) for y in range(2011,2018) for i in range(26)]\n",
    "submission_dates = ['{}{:02d}{:02d}'.format(date.year, date.month, date.day) for date in submission_dates]\n",
    "submission_dates = [datetime.strptime(str(d), \"%Y%m%d\") for d in submission_dates]\n",
    "submission_dates = pd.Series(submission_dates)\n",
    "\n",
    "#\n",
    "# Create list of target dates corresponding to submission dates in YYYYMMDD format\n",
    "#\n",
    "target_dates = pd.Series([get_target_date('{}{:02d}{:02d}'.format(date.year, date.month, date.day), target_horizon) for date in submission_dates])\n",
    "\n",
    "# Find all unique target day-month combinations\n",
    "target_day_months = pd.DataFrame({'month' : target_dates.dt.month, \n",
    "                                  'day': target_dates.dt.day}).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Choose regression parameters\n",
    "#\n",
    "# Record standard settings of these parameters\n",
    "setting = \"knn_regression\"\n",
    "if setting == \"knn_regression\":\n",
    "    base_col = clim_col\n",
    "    x_cols = ['knn1','ones']\n",
    "    margin_in_days = None\n",
    "    # columns to group by when fitting regressions (a separate regression\n",
    "    # is fit for each group); use ['ones'] to fit a single regression to all points\n",
    "    group_by_cols = ['lat', 'lon']\n",
    "    # anom_scale_col: multiply anom_col by this amount prior to prediction\n",
    "    # (e.g., 'ones' or anom_inv_std_col)\n",
    "    anom_scale_col = anom_inv_std_col\n",
    "    # pred_anom_scale_col: multiply predicted anomalies by this amount\n",
    "    # (e.g., 'ones' or anom_inv_std_col)\n",
    "    pred_anom_scale_col = anom_scale_col\n",
    "elif setting == \"stepwise_no_model_selection\":\n",
    "    base_col = clim_col\n",
    "    x_cols = default_stepwise_candidate_predictors(gt_id, target_horizon, hindcast=False) + ['knn1']\n",
    "    margin_in_days = 56\n",
    "    group_by_cols = ['lat', 'lon']\n",
    "    anom_scale_col = anom_inv_std_col\n",
    "    pred_anom_scale_col = anom_scale_col\n",
    "\n",
    "print x_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Default regression parameter values\n",
    "#\n",
    "# choose first year to use in training set\n",
    "first_train_year = 1948 if gt_id == 'contest_precip' else 1979\n",
    "# specify regression model\n",
    "fit_intercept = False\n",
    "model = linear_model.LinearRegression(fit_intercept=fit_intercept)\n",
    "\n",
    "#\n",
    "# Prepare target and feature data\n",
    "#\n",
    "relevant_cols = set(x_cols+[base_col,clim_col,anom_col,'sample_weight','target',\n",
    "                    'start_date','lat','lon','year','ones']+group_by_cols)\n",
    "# Create dataset with relevant columns only; otherwise the dataframe is too big\n",
    "tic()\n",
    "print \"Loading date and lat lon date data\"\n",
    "data_dir = os.path.join(\"results\", experiment, \"shared\", gt_id + \"_\" + target_horizon)\n",
    "date_data = pd.read_hdf(os.path.join(data_dir, \"date_data-\" + gt_id + \"_\" + target_horizon + \".h5\"))\n",
    "date_data['year'] = date_data.start_date.dt.year\n",
    "lat_lon_date_data = pd.read_hdf(\n",
    "    os.path.join(data_dir, \"lat_lon_date_data-\" + gt_id + \"_\" + target_horizon + \".h5\"))\n",
    "toc()\n",
    "# Restrict data to years >= first_train_year\n",
    "tic()\n",
    "print \"Restricting data to years >= {}\".format(first_train_year)\n",
    "lat_lon_date_data = lat_lon_date_data.loc[lat_lon_date_data.start_date.dt.year >= first_train_year]\n",
    "date_data = date_data.loc[date_data.year >= first_train_year]\n",
    "toc()\n",
    "# Drop rows with missing values for any relevant column \n",
    "tic()\n",
    "print \"Dropping rows with missing values for any relevant columns\"\n",
    "relevant_lat_lon_date_cols = list(set(lat_lon_date_data.columns.tolist()) & relevant_cols)\n",
    "lat_lon_date_data.dropna(subset = relevant_lat_lon_date_cols, inplace = True)\n",
    "relevant_date_cols = list(set(date_data.columns.tolist()) & relevant_cols)\n",
    "date_data.dropna(subset = relevant_date_cols, inplace = True)\n",
    "toc()\n",
    "# Add supplementary columns\n",
    "tic()\n",
    "print \"Adding supplementary columns\"\n",
    "lat_lon_date_data['anom_inv_sqrt_2nd_mom'] = 1.0/np.sqrt(\n",
    "    lat_lon_date_data.groupby('start_date')[anom_col].transform('mean')**2\n",
    "    + lat_lon_date_data.groupby('start_date')[anom_col].transform('var',ddof=0))\n",
    "lat_lon_date_data['ones'] = 1.0\n",
    "lat_lon_date_data['zeros'] = 0.0\n",
    "# To minimize the mean-squared error between predictions of the form\n",
    "# (f(x_cols) + base_col - clim_col) * pred_anom_scale_col\n",
    "# and a target of the form anom_col * anom_scale_col, we will\n",
    "# estimate f using weighted least squares with datapoint weights\n",
    "# pred_anom_scale_col^2 and effective target variable \n",
    "# anom_col * anom_scale_col / pred_anom_scale_col + clim_col - base_col\n",
    "lat_lon_date_data['sample_weight'] = lat_lon_date_data[pred_anom_scale_col]**2\n",
    "lat_lon_date_data['target'] = (lat_lon_date_data[clim_col] - lat_lon_date_data[base_col] + \n",
    "                               lat_lon_date_data[anom_col] * lat_lon_date_data[anom_scale_col] / \n",
    "                              (lat_lon_date_data[pred_anom_scale_col]+(lat_lon_date_data[pred_anom_scale_col]==0)))\n",
    "toc()\n",
    "\n",
    "# Load KNN data\n",
    "tic()\n",
    "print \"Loading KNN data\"\n",
    "past_days = 60\n",
    "days_early = 337 if target_horizon == \"34w\" else 323\n",
    "max_nbrs = 20\n",
    "knn_dir = os.path.join(\"data\", \"dataframes\")\n",
    "knn_data = pd.read_hdf(\n",
    "    os.path.join(knn_dir, \n",
    "                 \"knn-{}-{}-days{}-early{}-maxnbrs{}.h5\".format(\n",
    "                     gt_id, target_horizon, past_days, days_early, max_nbrs)))\n",
    "relevant_knn_cols = list(set(knn_cols) & relevant_cols)\n",
    "# Divide knn anomalies by std dev across grid cells\n",
    "knn_data[relevant_knn_cols] /= knn_data.groupby([\"start_date\"])[relevant_knn_cols].transform('std')\n",
    "toc()\n",
    "\n",
    "# Restrict data to relevant columns\n",
    "tic()\n",
    "print \"Merge datasets\"\n",
    "relevant_lat_lon_date_cols = list(set(lat_lon_date_data.columns.tolist()) & relevant_cols)\n",
    "data = lat_lon_date_data.loc[:, relevant_lat_lon_date_cols]\n",
    "relevant_knn_cols = list(set(knn_data.columns.tolist()) & relevant_cols)\n",
    "data = pd.merge(data, knn_data[relevant_knn_cols],\n",
    "                on=[\"start_date\",\"lat\",\"lon\"], how=\"left\")\n",
    "data = pd.merge(data, date_data[relevant_date_cols],\n",
    "                on=\"start_date\", how=\"left\")\n",
    "print data.head()\n",
    "del lat_lon_date_data\n",
    "del knn_data\n",
    "del date_data\n",
    "toc()\n",
    "\n",
    "# Print warning if not all x columns were included\n",
    "s = [x for x in x_cols if x not in data.columns.tolist()]\n",
    "if s:\n",
    "    print \"These x columns were not found:\"\n",
    "    print s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = 8\n",
    "# For the target (day, month) combination, fit leave-one-year regression model \n",
    "# on training set subsetted to relevant margin and generate predictions for each \n",
    "# held-out year\n",
    "prediction_func = rolling_linear_regression_wrapper\n",
    "all_preds = pd.DataFrame()\n",
    "\n",
    "generic_year = 2011\n",
    "for day_month in target_day_months.itertuples():\n",
    "    # Get target date on generic as a datetime object\n",
    "    target_date_obj = datetime.strptime('{}{:02d}{:02d}'.format(\n",
    "        generic_year, day_month.month, day_month.day), \"%Y%m%d\")\n",
    "    print 'day_month={}, target={}'.format(day_month, target_date_obj)\n",
    "    # Get number of days between start date of observation period used for prediction\n",
    "    # (2 weeks ahead) and start date of target period (2 or 4 weeks ahead) + 1 day do \n",
    "    # to practical constraints of submission\n",
    "    start_delta = get_start_delta(target_horizon) # 29 or 43\n",
    "    # Create template for held-out years: each held-out year will run from \n",
    "    # last_train_date + 1 on that year (inclusive) through last_train_date\n",
    "    # of the next year (inclusive)\n",
    "    last_train_date = target_date_obj - timedelta(start_delta)\n",
    "    if margin_in_days is not None:\n",
    "        tic()\n",
    "        sub_data = month_day_subset(data, target_date_obj, margin_in_days)\n",
    "        toc()\n",
    "    else:\n",
    "        sub_data = data\n",
    "    tic()\n",
    "    preds = apply_parallel(sub_data.groupby(group_by_cols),\n",
    "                           prediction_func, num_cores,\n",
    "                           x_cols=x_cols, \n",
    "                           base_col=base_col, \n",
    "                           clim_col=clim_col, \n",
    "                           anom_col=anom_col, \n",
    "                           last_train_date=last_train_date)\n",
    "    preds = preds.reset_index() \n",
    "    # Only keep the predictions from the target day and month\n",
    "    preds = preds[(preds.start_date.dt.day == target_date_obj.day) & \n",
    "                  (preds.start_date.dt.month == target_date_obj.month)]\n",
    "    # Concatenate predictions\n",
    "    all_preds = pd.concat([all_preds, preds])\n",
    "    toc()\n",
    "    #---------------\n",
    "    # Evaluate only on target dates                                                        \n",
    "    #---------------\n",
    "    tic()\n",
    "    skills = get_col_skill(\n",
    "        all_preds[all_preds.start_date.isin(target_dates)], \n",
    "        \"truth\", \"forecast\", time_average = False)\n",
    "    print \"running mean skill = {}\".format(skills.mean())\n",
    "    toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------\n",
    "# Evaluate on all target dates                                                        \n",
    "#---------------\n",
    "tic()\n",
    "skills = get_col_skill(\n",
    "    all_preds[all_preds.start_date.isin(target_dates)], \n",
    "    \"truth\", \"forecast\", time_average = False)\n",
    "print \"overall mean skill = {}\".format(skills.mean())\n",
    "#---------------\n",
    "# Evaluate on contest year                                                      \n",
    "#---------------\n",
    "skills = get_col_skill(\n",
    "    all_preds[all_preds.start_date.isin(target_dates) & (all_preds.start_date >= \n",
    "              get_target_date(\"20170418\", target_horizon))], \n",
    "    \"truth\", \"forecast\", time_average = False)\n",
    "print \"2017-2018 mean skill = {}\".format(skills.mean())\n",
    "toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Save target date predictions to file\n",
    "#\n",
    "\n",
    "# Ensure hash randomization turned off for reproducibility\n",
    "PYTHONHASHSEED=0\n",
    "# Define a compact string identifier for experiment parameters\n",
    "param_str = str(abs(hash((base_col, frozenset(x_cols), margin_in_days, frozenset(group_by_cols),\n",
    "                         anom_scale_col, pred_anom_scale_col))))\n",
    "# Create directory for storing results\n",
    "outdir = os.path.join('results',experiment,'2011-2018',gt_id+'_'+target_horizon,param_str)\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "# Write predictions to file; record the dependency on the type of knn features integrated\n",
    "# into the feature set in the file name\n",
    "preds_file = os.path.join(\n",
    "    outdir,'preds-{}-{}-days{}-early{}.h5'.format(gt_id,target_horizon,past_days,days_early))\n",
    "print \"Saving predictions to \"+preds_file; tic()\n",
    "all_preds[all_preds.start_date.isin(target_dates)].to_hdf(preds_file, key=\"data\", mode=\"w\"); toc()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
